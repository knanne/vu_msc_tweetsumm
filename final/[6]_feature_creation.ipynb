{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create elongation regular expression match\n",
    "elongated = re.compile('([a-zA-Z])\\\\1{3,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2902: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2295892, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_table('data/final/event_panama_papers_data.txt', sep='\\t', encoding='utf-8', header=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fill NaN in some features\n",
    "df['user_id_verified'] = df['user_id_verified'].astype(object).replace(np.nan, 0)\n",
    "#encode some features\n",
    "df['user_id_verified'] = df['user_id_verified'].apply(lambda d: [0,1][d==True]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure to handle text as string\n",
    "df['text'] = df['text'].astype('str')\n",
    "df['text_clean'] = df['text_clean'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add twitter features\n",
    "df['count_words'] = df['text_clean'].apply(lambda text: len([w for w in text.split()]))\n",
    "df['count_stops'] = df.apply(lambda doc: len(doc['text'].split()) - len(doc['text_clean'].split()), axis=1)\n",
    "df['count_characters'] = df['text'].apply(lambda text: len(str(text)))\n",
    "df['count_non_characters'] = df['text'].apply(lambda text: len(re.sub('[\\w+!@#$%&;:,.?\\/\\-“”’`\"\\'()|]', '', text).strip()))\n",
    "df['count_upper'] = df['text'].apply(lambda text: len([l for l in ' '.join([w for w in text.split() if not w.startswith(('#', '@'))]) if l.isupper()]))\n",
    "df['bool_question'] = df['text'].apply(lambda text: 1 if '?' in text else 0)\n",
    "df['bool_elongation'] = df['text'].apply(lambda text: 1 if bool(elongated.search(text)) else 0)\n",
    "df['bool_ellipsis'] = df['text'].apply(lambda text: 1 if any(x in text for x in ('...', '…')) else 0)\n",
    "df['lexical_diversity'] = df['text'].apply(lambda text: len(set(text.split())) / len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add event features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get query grams\n",
    "query_terms = df['query'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compress series of tweet texts to list\n",
    "tweets = [ str(d) for d in df[df['is_retweet'] == False]['text_clean']]\n",
    "#extract tokens as list\n",
    "tokens = [ w for t in tweets for w in t.split()]\n",
    "#construct term counter\n",
    "for w in [tokens]:\n",
    "    termcounts = Counter(w)\n",
    "#save top k = 100 most frequent terms\n",
    "topk_terms = termcounts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['query_grams_coverage'] = df['text_clean'].apply(lambda text: len([token for token in text.split() if token.lower() in query_terms]) / len(query_terms))\n",
    "df['topk_terms_coverage'] = df['text_clean'].apply(lambda text: len([token for token in text.split() if token.lower() in topk_terms]) / len(topk_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(907440, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use only non retweets for bag of words statistics\n",
    "df_nonrt = df[df['is_retweet'] == False][['master_id', 'text_clean']]\n",
    "df_nonrt = df_nonrt.reset_index(drop=True)\n",
    "df_nonrt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize sklearn vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=20, max_features=1000, stop_words='english')\n",
    "#create matrix of tfidf counts\n",
    "#not considering retweets, due to redundancy skew\n",
    "Xtfidf = tfidf_vectorizer.fit_transform(df_nonrt['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get mean tfidf for each doc\n",
    "Xtfidf_means = Xtfidf.mean(axis=1)\n",
    "df_tfidf_means = pd.DataFrame(Xtfidf_means, columns=['tfidf_mean'])\n",
    "#get sum tfidf for each doc\n",
    "Xtfidf_sums = Xtfidf.sum(axis=1)\n",
    "df_tfidf_sums = pd.DataFrame(Xtfidf_sums, columns=['tfidf_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tfidf_stats = pd.concat([df_tfidf_means, df_tfidf_sums], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add tfidf sum, mean as features\n",
    "df_nonrt = pd.merge(df_nonrt, df_tfidf_stats, how='inner', left_index=True, right_index=True)\n",
    "#join back to full datatset\n",
    "df = pd.merge(df, df_nonrt[['master_id', 'tfidf_sum', 'tfidf_mean']], how='left', on='master_id')\n",
    "#retweet feature values are set to NaN\n",
    "#because retweets will exempt from modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate event centroid using tfidf mean of all columns (1000 top terms)\n",
    "Xtfidf_centroid = Xtfidf.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute pairwise distance for each doc to centroid using cosine similarity equation\n",
    "Xtfidf_centroid_cosdistance = pairwise_distances(X=Xtfidf, Y=Xtfidf_centroid, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_centroid_distance = pd.DataFrame(Xtfidf_centroid_cosdistance, columns=['event_centroid_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add centroid distance as feature\n",
    "df_nonrt = pd.merge(df_nonrt, df_centroid_distance, how='inner', left_index=True, right_index=True)\n",
    "#join back to full datatset\n",
    "df = pd.merge(df, df_nonrt[['master_id', 'event_centroid_distance']], how='left', on='master_id')\n",
    "#retweet centroid values are set to NaN\n",
    "#because retweets will exempt from modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#classify parts of speech, named entities using nltk classifier\n",
    "#aggregate pos, ne counts, add counts and dictionary of parts/entities to database\n",
    "#this is a slow process, should be redesigned\n",
    "df_nespos = pd.DataFrame()\n",
    "\n",
    "#define function for nltk tree mining\n",
    "def getnes(tree):\n",
    "    ne = []\n",
    "    for node in tree:\n",
    "        if type(node) is nltk.Tree:\n",
    "            label = node.label()\n",
    "            s = ''\n",
    "            for node in node:\n",
    "                s = (s + ' ' + node[0].lower()).lstrip()\n",
    "            ne.append([label, s])\n",
    "    return ne\n",
    "\n",
    "for i,doc in df.iterrows():\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        complete = round((i/df.shape[0])*100, 2)\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(str(doc['text_clean']))\n",
    "    \n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    pos_cntr = Counter(list(dict(pos).values()))\n",
    "    pos_data = dict(pos_cntr)\n",
    "    pos_cnt = sum(pos_data.values())\n",
    "    \n",
    "    tree = nltk.ne_chunk(pos)\n",
    "    nes = getnes(tree)\n",
    "    nes_cntr = Counter(list(dict(nes).keys()))\n",
    "    nes_data = dict(nes_cntr)\n",
    "    nes_cnt = sum(nes_data.values())\n",
    "    \n",
    "    row = [pos_cnt, nes_cnt, pos_data, nes_data]\n",
    "    row = pd.Series(row, index=['pos_cnt', 'nes_cnt', 'pos_data', 'nes_data'])\n",
    "    row = pd.DataFrame(row).T\n",
    "    df_nespos = df_nespos.append(row, ignore_index=True)\n",
    "    \n",
    "df = pd.merge(df, df_nespos, left_index=True, right_index=True)\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get all parts of speach counts, add as features\n",
    "all_pos = []\n",
    "for i,doc in df.iterrows():\n",
    "    all_pos.extend(list(doc['pos_data'].keys()))\n",
    "\n",
    "#get unique pos types\n",
    "all_pos = list(set(all_pos))\n",
    "\n",
    "#create pos feature df\n",
    "df_pos = pd.DataFrame()\n",
    "pos_cols = ['pos_cnt_'+pos for pos in all_pos] \n",
    "\n",
    "#update pos counts\n",
    "for i,doc in df.iterrows():    \n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        complete = round((i/df.shape[0])*100, 2)\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    #create empty dictionary with keys\n",
    "    pos_dict = dict.fromkeys(all_pos)\n",
    "    \n",
    "    for pos in doc['pos_data'].keys():\n",
    "        pos_dict[pos] = doc['pos_data'].get(pos)\n",
    "    \n",
    "    row = list(pos_dict.values())\n",
    "    row = pd.Series(row, index=pos_cols)\n",
    "    row = pd.DataFrame(row).T\n",
    "    df_pos = df_pos.append(row, ignore_index=True)\n",
    "    \n",
    "df = pd.merge(df, df_pos, left_index=True, right_index=True)\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get all named entities counts, add as features\n",
    "all_nes = []\n",
    "for i,doc in df.iterrows():\n",
    "    all_nes.extend(list(doc['nes_data'].keys()))\n",
    "\n",
    "#get unique\n",
    "all_nes = list(set(all_nes))\n",
    "\n",
    "#create nes feature df\n",
    "df_nes = pd.DataFrame()\n",
    "ne_cols = ['ne_cnt_'+ne for ne in all_nes] \n",
    "    \n",
    "#update pos counts\n",
    "for i,doc in df.iterrows():    \n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        complete = round((i/df.shape[0])*100, 2)\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    #create empty dictionary with keys\n",
    "    nes_dict = dict.fromkeys(all_nes)\n",
    "    \n",
    "    for ne in doc['nes_data'].keys():\n",
    "        nes_dict[ne] = doc['nes_data'].get(ne)\n",
    "    \n",
    "    row = list(nes_dict.values())\n",
    "    row = pd.Series(row, index=ne_cols)\n",
    "    row = pd.DataFrame(row).T\n",
    "    df_nes = df_nes.append(row, ignore_index=True)\n",
    "    \n",
    "df = pd.merge(df, df_nes, left_index=True, right_index=True)\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2295892, 37)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>master_id</th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>entities_count_hashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>count_upper</th>\n",
       "      <th>bool_question</th>\n",
       "      <th>bool_elongation</th>\n",
       "      <th>bool_ellipsis</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>query_grams_coverage</th>\n",
       "      <th>topk_terms_coverage</th>\n",
       "      <th>tfidf_sum</th>\n",
       "      <th>tfidf_mean</th>\n",
       "      <th>event_centroid_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>716414944355287040</td>\n",
       "      <td>2016-04-03 00:00:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Environmentalists Call For No New Offshore Dri...</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.608957</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.877434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   master_id          twitter_id           created_at coordinates  \\\n",
       "0          0  716414944355287040  2016-04-03 00:00:07         NaN   \n",
       "\n",
       "                                                text tweet_type is_retweet  \\\n",
       "0  Environmentalists Call For No New Offshore Dri...       text      False   \n",
       "\n",
       "   favorite_count  retweet_count  entities_count_hashtags  \\\n",
       "0               0              0                        0   \n",
       "\n",
       "            ...             count_upper  bool_question  bool_elongation  \\\n",
       "0           ...                      13              0                0   \n",
       "\n",
       "   bool_ellipsis lexical_diversity  query_grams_coverage  topk_terms_coverage  \\\n",
       "0              0               1.0                   0.2                  0.0   \n",
       "\n",
       "   tfidf_sum  tfidf_mean  event_centroid_distance  \n",
       "0   1.608957    0.001609                 0.877434  \n",
       "\n",
       "[1 rows x 37 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save event data with features\n",
    "#set nulls to 0 (in case of pos/ne counts)\n",
    "df.to_csv('data/final/event_panama_papers_data.txt', sep='\\t', encoding='utf-8', header=True, index=False, na_rep=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
