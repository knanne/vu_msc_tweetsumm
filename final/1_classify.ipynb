{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "We model the quality of tweets as well as the likelihood of a tweet to contain news.\n",
    "\n",
    "### Feature Creation\n",
    "\n",
    "- event-independent\n",
    "    - meta\n",
    "    - content\n",
    "    - social\n",
    "        - tweet\n",
    "        - user\n",
    "    - lexical\n",
    "    - semantic\n",
    "- event-dependent\n",
    "    - time\n",
    "    - content\n",
    "    - semantic\n",
    "    \n",
    "### Evaluate Features\n",
    "\n",
    "- using $\\chi^2$ test of independence\n",
    "    \n",
    "### Setup Pipeline\n",
    "- Multinomial Naive Bayes\n",
    "    - \n",
    "- K Nearest Neighbor\n",
    "    - \n",
    "- Decision Tree\n",
    "    - Decision trees learn branching rules from features.\n",
    "- Random Forest\n",
    "    - A random forest is a collection of decision trees. Each tree is trained using a random sample of the dataset. The prediction is an average of all the predictions made by the trees.\n",
    "- Support Vector Classifier\n",
    "    - \n",
    "\n",
    "### Run Pipeline\n",
    "- specify class to predict\n",
    "    - News (0,1)\n",
    "    \n",
    "*We discard those values which were annotated as unclear and model on those which were identified as news or non-news content. This clearer sepearation in our data increase the accuracy of the model, although requires more data to be annotated.*\n",
    "\n",
    "### Results\n",
    "\n",
    "- graph each model's results by three measures\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "\n",
    "### Evaluate Results\n",
    "- interpretation of results\n",
    "    - Accuracy\n",
    "        - Classification accuracy is the number of correct predictions made divided by the total number of predictions made. This is studied as the jaccard similarity score.\n",
    "    - Precision\n",
    "        - fraction of correctly predictive positives to all predicted as positives: $\\frac{true positives}{true postivies + false positives}$\n",
    "    - Recall\n",
    "        - fraction of correctly predicted positives to all positives: $\\frac{true posisitves}{true positives + false negatives}$\n",
    "    - F1 Score\n",
    "        - weighted average of precision and recall, from 0 to 1: $\\frac{2 \\cdot (precision \\cdot recall)}{precision + recall}$\n",
    "\n",
    "Accepting certain results from the above can depend on application. Specifically, we try to combat the [**accuracy paradox**](https://en.wikipedia.org/wiki/Accuracy_paradox) which is relevant to datsets of unbalanced class distribution. Given a dataset of large class unbalance, the classifier will predict the dominant class majority of the time and result in high accuracy. Although this accuracry is misleading because it simply represents the underlying class distribution. Alternatively, we look at further measures of evaluation.\n",
    "\n",
    "If the applications requires high degree of certainty for those positive predictions actually being positive then **high precsion** is most important. If the application requires the prediciton of as many positive examples as possible then **high recall** will be most important. The former application could accept some positive examples being misclassified as negative and the latter could accept some negative example being misclassified as positive.\n",
    "\n",
    "For the application of event summarization on Twitter, we consider the scenario of needing learn as much newsworthy information as possible in a short amount of time. Our process should present only only that content only which is of high probability of being newsworthy and ignore that which is not. Therefore we are interested in **maximizing precision**, and can likely dismiss recall due to the volume of data we have access to.\n",
    "\n",
    "In addition to looking at precision, we apply the technique of **undersampling** to create a more balanced dataset for classifying. By simply dropping some of data from the dominant class we can boost the representation of the underrepresented class. This will in turn create a more meaninful accuracy score. This technique is available due to the volume of data we are able to annotate through crowdsourcing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import nltk\n",
    "import html\n",
    "import string\n",
    "import ast\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import crowdsourced annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411, 69)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REAL DATA\n",
    "#cf = pd.read_csv('data/final/tweet_annotations_agg.csv', sep='\\t', encoding='utf-8', header=0)\n",
    "#cf.shape\n",
    "#TEST DATA\n",
    "cf = pd.read_csv('data/final/cf_report_manualoriginal.csv',  sep=',', encoding='utf-8', header=0)\n",
    "cf.rename(columns={'master_id' : 'event_id'}, inplace=True)\n",
    "cf.drop_duplicates(subset=['event', 'event_id'], inplace=True)\n",
    "cf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import event data\n",
    "- for each event name import event data\n",
    "- create dictionary of { Event Name : Pandas DataFrame }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254461, 35)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "for event_name in cf['event'].unique():\n",
    "    event_filename = re.sub(\"\\W+\", \"\", event_name.strip())\n",
    "    data[event_name] = pd.read_csv('data/final/backup/event_%s_data.txt' % event_filename, sep='\\t', encoding='utf-8', header=0, parse_dates=['created_at'], dtype={'twitter_id' : 'str'})\n",
    "    \n",
    "allevents = list(data.values())\n",
    "\n",
    "df = pd.concat(allevents)\n",
    "df.drop_duplicates(subset='twitter_id', inplace=True)\n",
    "#df.rename(columns={'master_id' : 'event_id'}, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge annotations to data\n",
    "- using event name and event id\n",
    "- integity of twitter id was lost during annotation (crowdflower did not support such a long integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 36)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, cf[['event', 'event_id', 'news']], how='inner', on=['event', 'event_id'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index('twitter_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### created clean text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#convert text to string\n",
    "#remove hashtag, mention symbols\n",
    "#remove any links\n",
    "df['text_clean'] = df['text'].apply(lambda text: ' '.join([(w[:w.find('http')] if 'http' in w else w) for w in html.unescape(str(text)).replace('#', '').replace('@', '').split()]))\n",
    "#initiate, apply tokenization\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "df['text_clean'] = df['text_clean'].apply(lambda text: ' '.join([w.lower() for w in tknzr.tokenize(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature list\n",
    "- **event-independent**\n",
    "    - **meta**\n",
    "        - source_web_client\n",
    "        - source_tweetdeck\n",
    "        - source_iphone\n",
    "        - source_android\n",
    "        - has_location\n",
    "    - **content**\n",
    "        - tweet_type\n",
    "        - possibly_sensitive\n",
    "        - count_entities_media\n",
    "        - media_per_word\n",
    "        - count_entities_urls\n",
    "        - urls_per_word\n",
    "        - count_entities_mentions\n",
    "        - mentions_per_word\n",
    "        - count_entities_hashtags\n",
    "        - hashtags_per_word\n",
    "    - **social**\n",
    "        - **tweet**\n",
    "            - is_retweet\n",
    "            - is_reply\n",
    "            - is_quoted_tweet\n",
    "            - favorite_count\n",
    "            - retweet_count\n",
    "            - engagements\n",
    "            - engagements_per_word\n",
    "            - favorite_count_eventstdz\n",
    "            - retweet_count_eventstdz\n",
    "            - engagements_count_eventstdz\n",
    "        - **user**\n",
    "            - user_default_profile\n",
    "            - user_default_profile_image\n",
    "            - user_verified\n",
    "            - user_statuses\n",
    "            - user_favourites\n",
    "            - user_followers\n",
    "            - user_friends\n",
    "            - user_listed\n",
    "            - user_bio_len\n",
    "            - user_reputation\n",
    "            - user_age_days\n",
    "            - user_follower_rate\n",
    "    - **lexical**\n",
    "        - count_characters\n",
    "        - count_non_characters\n",
    "        - count_upper\n",
    "        - count_tokens\n",
    "        - count_stops\n",
    "        - question\n",
    "        - elongation\n",
    "        - ellipsis\n",
    "        - lexical_diversity\n",
    "        - sfpp\n",
    "        - pfpp\n",
    "        - count_named_entities\n",
    "        - unique_named_entities\n",
    "        - count_parts_of_speech\n",
    "        - unique_parts_of_speech\n",
    "    - **semantic**\n",
    "        - text_sentiment_positive\n",
    "        - text_sentiment_negative\n",
    "        - text_sentiment_subjective\n",
    "- **event-dependent**\n",
    "    - **pulse**\n",
    "        - frequency_1min\n",
    "        - rollmean_frequency_win5min\n",
    "        - rollmean_frequency_deviation\n",
    "    - **content**\n",
    "        - media_weight\n",
    "        - url_weight\n",
    "        - mention_weight\n",
    "        - hashtag_weight\n",
    "    - **semantic**\n",
    "        - term_weight\n",
    "        - tfidf_sum\n",
    "        - tfidf_mean\n",
    "        - event_centroid_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-Independent\n",
    "### Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['source_web_client'] = df['tweet_type'].apply(lambda s: [0,1][s=='Twitter Web Client'])\n",
    "df['source_tweetdeck'] = df['tweet_type'].apply(lambda s: [0,1][s=='TweetDeck'])\n",
    "df['source_iphone'] = df['tweet_type'].apply(lambda s: [0,1][s=='Twitter for iPhone'])\n",
    "df['source_android'] = df['tweet_type'].apply(lambda s: [0,1][s=='Twitter for Android'])\n",
    "df['has_location'] = df['longitude'].apply(lambda l: [1,0][np.isnan(l)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['tweet_type'] = df['tweet_type'].apply(lambda d: [0,1][d=='media'])\n",
    "df['possibly_sensitive'] = df['possibly_sensitive'].apply(lambda d: [0,1][d==True])\n",
    "df['count_entities_media'] = df['entities_media'].apply(lambda media: len(media))\n",
    "df['media_per_word'] = df.apply(lambda row: row['count_entities_media'] / len(row['text'].split()), axis=1)\n",
    "df['count_entities_urls'] = df['entities_urls'].apply(lambda urls: len(urls))\n",
    "df['urls_per_word'] = df.apply(lambda row: row['count_entities_urls'] / len(row['text'].split()), axis=1)\n",
    "df['count_entities_mentions'] = df['entities_mentions'].apply(lambda mentions: len(mentions))\n",
    "df['mentions_per_word'] = df.apply(lambda row: row['count_entities_mentions'] / len(row['text'].split()), axis=1)\n",
    "df['count_entities_hashtags'] = df['entities_hashtags'].apply(lambda tags: len(tags))\n",
    "df['hashtags_per_word'] = df.apply(lambda row: row['count_entities_hashtags'] / len(row['text'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social\n",
    "#### Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#following created by twitter\n",
    "    #is_retweet\n",
    "    #is_reply\n",
    "    #is_quoted_tweet\n",
    "    #favorite_count\n",
    "    #retweet_count\n",
    "df['engagements'] = df['favorite_count'] + df['retweet_count']\n",
    "df['engagements_per_word'] = df.apply(lambda row: row['engagements'] / len(row['text'].split()), axis=1)\n",
    "#standardize tweet social data to event distribution\n",
    "#NOTE: this is only relative to training documents\n",
    "df['favorite_count_eventstdz'] = df.groupby('event')['favorite_count'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df['retweet_count_eventstdz'] = df.groupby('event')['retweet_count'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df['engagements_eventstdz'] = df.groupby('event')['engagements'].transform(lambda x: (x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['user_bio_len'] = df['user_description'].apply(lambda bio: len(str(bio)))\n",
    "df['user_verified'] = df['user_verified'].apply(lambda d: [0,1][d==True])\n",
    "df['user_default_profile'] = df['user_default_profile'].apply(lambda d: [0,1][d==True])\n",
    "df['user_default_profile_image'] = df['user_default_profile_image'].apply(lambda d: [0,1][d==True])\n",
    "df['user_reputation'] = df['user_followers'] / (df['user_friends'])\n",
    "df['user_reputation'].replace(np.inf, np.nan, inplace=True)\n",
    "df['user_age_days'] = df.apply(lambda doc: (doc['created_at'] - pd.to_datetime(doc['user_created_at'])).days, axis=1)\n",
    "df['user_follower_rate'] = df['user_followers'] / df['user_age_days']\n",
    "df['user_follower_rate'].replace(np.inf, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create stop words list\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "punct = list(string.punctuation)\n",
    "punct.extend(['...', '..', '…', '”', '“', '.@', 'RT'])\n",
    "stop.extend(punct)\n",
    "#create elongated regular expression (>= 3 letters in a row)\n",
    "elongation = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n",
    "\n",
    "df['count_characters'] = df['text'].apply(lambda text: len(str(text)))\n",
    "df['count_non_characters'] = df['text'].apply(lambda text: len(re.sub('[\\w+!@#$%&;:,.?\\/\\-“”’`\"\\'()|]', '', text).strip()))\n",
    "df['count_upper'] = df['text'].apply(lambda text: len([l for l in ' '.join([w for w in text.split() if not w.startswith(('#', '@'))]) if l.isupper()]))\n",
    "df['count_tokens'] = df['text_clean'].apply(lambda text: len(text.split()))\n",
    "df['mean_token_length'] = df['text_clean'].apply(lambda text: np.mean([len(t) for t in text.split()]))\n",
    "df['mean_token_frequency'] = df['text_clean'].apply(lambda text: np.mean(list(Counter(text.split()).values())))\n",
    "df['count_tokens_stopped'] = df['text_clean'].apply(lambda text: len([t for t in text.split() if t not in stop]))\n",
    "df['count_stops'] = df['text_clean'].apply(lambda text: len([t for t in text.split() if t in stop]))\n",
    "df['count_question_marks'] = df['text_clean'].apply(lambda text: text.split().count('?'))\n",
    "df['elongation'] = df['text_clean'].apply(lambda text: [0,1][bool(elongation.search(text))])\n",
    "df['ellipsis'] = df['text_clean'].apply(lambda text: 1 if any(x in text for x in ('...', '…')) else 0)\n",
    "df['lexical_diversity'] = df['text_clean'].apply(lambda text: len(set(text.split())) / len(text.split()))\n",
    "# singular first person pronoun\n",
    "sfpp = ['i', 'i\\'m', 'me', 'mine', 'my', 'myself']\n",
    "df['sfpp'] = df['text_clean'].apply(lambda text: 1 if any(t.lower() in sfpp for t in text.split()) else 0)\n",
    "# plural first person pronoun\n",
    "pfpp = ['we', 'we\\'re', 'ours', 'our', 'ourselves']\n",
    "df['pfpp'] = df['text_clean'].apply(lambda text: 1 if any(t.lower() in pfpp for t in text.split()) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text_sentiment_polarity'] = df['text_clean'].apply(lambda text: TextBlob(str(text)).sentiment.polarity)\n",
    "df['text_sentiment_positive'] = df['text_sentiment_polarity'].apply(lambda s: [0,abs(s)][s > 0])\n",
    "df['text_sentiment_negative'] = df['text_sentiment_polarity'].apply(lambda s: [0,abs(s)][s < 0])\n",
    "df['text_sentiment_subjectivity'] = df['text_clean'].apply(lambda text: TextBlob(str(text)).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-Dependent\n",
    "### Pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['frequency_1min'] = np.nan\n",
    "df['rollmean_frequency_win5min'] = np.nan\n",
    "df['rollmean_frequency_deviation'] = np.nan\n",
    "\n",
    "for dfevent in allevents:\n",
    "\n",
    "    dfevent = dfevent[['twitter_id', 'created_at']]\n",
    "    \n",
    "    #event frequency distribution\n",
    "    event_dist = dfevent.set_index('created_at').groupby([pd.TimeGrouper(freq='min')])['twitter_id'].agg(['count'])\n",
    "    \n",
    "    #fill gaps\n",
    "    timeframe = pd.date_range(event_dist.index.min(), event_dist.index.max(), freq='T')\n",
    "    event_dist = event_dist.reindex(timeframe, fill_value=0)\n",
    "\n",
    "    #rolling mean and deviation from\n",
    "    event_dist['mean'] = event_dist['count'].rolling(window=5).agg(['mean'])\n",
    "    event_dist['deviation'] = event_dist['count'] - event_dist['mean']    \n",
    "\n",
    "    #create temp df of twitter ids, created_at rounded to second (for joining freq. dist. with data)\n",
    "    event_temp = dfevent[['twitter_id', 'created_at']]\n",
    "    event_temp['created_at'] = event_temp['created_at'].apply(lambda x: x.replace(second=0))\n",
    "    event_temp.set_index(['created_at'], inplace=True)\n",
    "\n",
    "    #join frequency distribution with data\n",
    "    event_dist = pd.merge(event_temp, event_dist[['count', 'mean', 'deviation']], how='left', left_index=True, right_index=True)\n",
    "    #set index back, remove rounded created at\n",
    "    event_dist.set_index('twitter_id', inplace=True)\n",
    "    \n",
    "    #standardize to event\n",
    "    event_dist['count'] = (event_dist['count'] - event_dist['count'].mean()) / event_dist['count'].std()\n",
    "    event_dist['mean'] = (event_dist['mean'] - event_dist['mean'].mean()) / event_dist['mean'].std()\n",
    "    event_dist['deviation'] = (event_dist['deviation'] - event_dist['deviation'].mean()) / event_dist['deviation'].std()\n",
    "    #event_dist['deviation'] = event_dist['deviation'].apply(lambda x: [0,x][x > 0])\n",
    "    #event_dist['deviation'] = event_dist['deviation'] / event_dist['deviation'].max()\n",
    "    #rename columns\n",
    "    event_dist.columns = ['frequency_1min','rollmean_frequency_win5min','rollmean_frequency_deviation']\n",
    "    \n",
    "    #update data cols with event dist. data (join on twitter id)\n",
    "    df.update(event_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content & Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['media_weight'] = np.nan\n",
    "df['url_weight'] = np.nan\n",
    "df['mention_weight'] = np.nan\n",
    "df['hashtag_weight'] = np.nan\n",
    "df['term_weight'] = np.nan\n",
    "df['tfidf_mean'] = np.nan\n",
    "df['event_centroid_distance'] = np.nan\n",
    "\n",
    "for dfevent in allevents:\n",
    "    dfevent = dfevent[['twitter_id', 'text', 'entities_media', 'entities_urls', 'entities_mentions', 'entities_hashtags']]\n",
    "\n",
    "    #clean text\n",
    "    dfevent['text_clean'] = dfevent['text'].apply(lambda text: ' '.join([(w[:w.find('http')] if 'http' in w else w) for w in html.unescape(str(text)).replace('#', '').replace('@', '').split()]))\n",
    "    dfevent['text_clean'] = dfevent['text_clean'].apply(lambda text: ' '.join([w.lower() for w in tknzr.tokenize(text)]))\n",
    "\n",
    "    #media weight\n",
    "    entities_lists = dfevent['entities_media'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "    allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "    entity_counts = Counter(allentities)\n",
    "    dfevent['media_weight'] = dfevent['entities_media'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "    dfevent['media_weight'] = (dfevent['media_weight'] - dfevent['media_weight'].mean()) / dfevent['media_weight'].std()\n",
    "\n",
    "    #url weight\n",
    "    entities_lists = dfevent['entities_urls'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "    allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "    entity_counts = Counter(allentities)\n",
    "    dfevent['url_weight'] = dfevent['entities_urls'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "    dfevent['url_weight'] = (dfevent['url_weight'] - dfevent['url_weight'].mean()) / dfevent['url_weight'].std()\n",
    "\n",
    "    #mention weight\n",
    "    entities_lists = dfevent['entities_mentions'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "    allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "    entity_counts = Counter(allentities)\n",
    "    dfevent['mention_weight'] = dfevent['entities_mentions'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "    dfevent['mention_weight'] = (dfevent['mention_weight'] - dfevent['mention_weight'].mean()) / dfevent['mention_weight'].std()\n",
    "\n",
    "    #hashtag weight\n",
    "    entities_lists = dfevent['entities_hashtags'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "    allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "    entity_counts = Counter(allentities)\n",
    "    dfevent['hashtag_weight'] = dfevent['entities_hashtags'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "    dfevent['hashtag_weight'] = (dfevent['hashtag_weight'] - dfevent['hashtag_weight'].mean()) / dfevent['mention_weight'].std()\n",
    "   \n",
    "    #term weight\n",
    "    tweets = [str(d) for d in dfevent['text_clean']]\n",
    "    tokens = [w for t in tweets for w in t.split() if w not in stop] \n",
    "    token_counts = Counter(tokens)\n",
    "    dfevent['term_weight'] = dfevent['text_clean'].apply(lambda text: sum([token_counts.get(t) for t in text.split() if t not in stop]))\n",
    "    dfevent['term_weight'] = (dfevent['term_weight'] - dfevent['term_weight'].mean()) / dfevent['term_weight'].std()\n",
    "        \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "    xtfidf = tfidf_vectorizer.fit_transform(dfevent['text_clean'])\n",
    "\n",
    "    #tfidf mean\n",
    "    xtfidf_means = xtfidf.mean(axis=1)\n",
    "    xtfidf_means = pd.DataFrame(xtfidf_means, columns=['tfidf_mean'])\n",
    "    dfevent = pd.merge(dfevent, xtfidf_means, how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    #tfidf centroid distance\n",
    "    xtfidf_centroid = xtfidf.mean(axis=0)\n",
    "    xtfidf_cosdistance = pairwise_distances(X=xtfidf, Y=xtfidf_centroid, metric='cosine')\n",
    "    xtfidf_cosdistance = pd.DataFrame(xtfidf_cosdistance, columns=['event_centroid_distance'])\n",
    "    dfevent = pd.merge(dfevent, xtfidf_cosdistance, how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    #set index back to twitter id\n",
    "    dfevent.set_index('twitter_id', inplace=True)\n",
    "    \n",
    "    #update columns in dataframe, on index (twitter_id)\n",
    "    df.update(dfevent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parts of Speech & Named Entities\n",
    "- using Stanford's [NERtagger](http://nlp.stanford.edu/software/CRF-NER.shtml) and [POStagger](http://nlp.stanford.edu/software/tagger.shtml) \n",
    "- interfacing through [NLTK python port](http://www.nltk.org/api/nltk.tag.html#nltk.tag.stanford.StanfordNERTagger)\n",
    "- basic implementation of using Stanford NER Tagger on Tweets\n",
    "    - with decode html characters (e.g. \"&amp;\")\n",
    "    - and remove hashtag, mention signs from entities\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "import html\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "stanford_NER = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "tweet = '.@DonaldTrump is a schmuck. He is in Iowa, with @HillaryClinton talking about bananas on @CNN. #Tweeting'\n",
    "\n",
    "tokens = tknzr.tokenize(html.unescape(str(tweet)).replace('#', '').replace('@', ''))\n",
    "\n",
    "ner = stanford_ner.tag(tokens)\n",
    "print(ner)\n",
    "```\n",
    "\n",
    "**result: **\n",
    "[('.', 'O'),\n",
    " ('DonaldTrump', 'PERSON'),\n",
    " ('is', 'O'),\n",
    " ('a', 'O'),\n",
    " ('schmuck', 'O'),\n",
    " ('.', 'O'),\n",
    " ('He', 'O'),\n",
    " ('is', 'O'),\n",
    " ('in', 'O'),\n",
    " ('Iowa', 'LOCATION'),\n",
    " (',', 'O'),\n",
    " ('with', 'O'),\n",
    " ('HillaryClinton', 'PERSON'),\n",
    " ('talking', 'O'),\n",
    " ('about', 'O'),\n",
    " ('bananas', 'O'),\n",
    " ('on', 'O'),\n",
    " ('CNN', 'ORGANIZATION'),\n",
    " ('.', 'O'),\n",
    " ('Tweeting', 'O')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger, StanfordPOSTagger\n",
    "from nltk.internals import find_jars_within_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### named entities\n",
    "- NOTE: used temp hack to get this working on windows see [github issue](https://github.com/nltk/nltk/issues/1237) and [stackoverflow responses](http://stackoverflow.com/questions/34361725/nltk-stanfordnertagger-noclassdeffounderror-org-slf4j-loggerfactory-in-windo/34403741#34403741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanford_NER = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "#NEEDED - temp hack fix until nltk update\n",
    "stanford_dir = stanford_NER._stanford_jar.rpartition('\\\\')[0] #gets existing jar directory\n",
    "stanford_jars = find_jars_within_path(stanford_dir) #finds all jars in directory and subdirectories\n",
    "stanford_NER._stanford_jar = ';'.join(stanford_jars) #join paths add all jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def getNE(tweet):\n",
    "    tokens = tknzr.tokenize(html.unescape(str(tweet)).replace('#', '').replace('@', ''))\n",
    "    ner = stanford_NER.tag(tokens)\n",
    "    return dict(ne for ne in ner if ne[1] != 'O')\n",
    "\n",
    "df['named_entities'] = df['text'].apply(lambda text: getNE(text))\n",
    "df['count_named_entities'] = df['named_entities'].apply(lambda ne: len(ne))\n",
    "df['unique_named_entities'] = df['named_entities'].apply(lambda ne: len(set(ne.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### expand/flatten named entities as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 939 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_nes = []\n",
    "for _,doc in df.iterrows():\n",
    "    all_nes.extend(list(doc['named_entities'].values()))\n",
    "\n",
    "all_nes = list(set(all_nes))\n",
    "\n",
    "#initialize empty dataframe, twitter_id by nes type columns, filled with 0's\n",
    "df_nes = pd.DataFrame(data=0, index=df.index, columns=all_nes)\n",
    "\n",
    "for twitter_id,tweet in df.iterrows():\n",
    "    \n",
    "    #get nes type counts\n",
    "    nes_counts = dict(Counter(list(dict(tweet['named_entities']).values())))\n",
    "    \n",
    "    #transform tweet nes type counts to uniform dataframe row\n",
    "    nes_dict = dict.fromkeys(all_nes)\n",
    "    for e in nes_counts.keys():\n",
    "        nes_dict[e] = nes_counts.get(e)\n",
    "    row = pd.Series(nes_dict, index=all_nes)\n",
    "    row = pd.DataFrame(row).T\n",
    "\n",
    "    #update nes feature dataframe\n",
    "    row['twitter_id'] = twitter_id\n",
    "    row.set_index('twitter_id', inplace=True)\n",
    "    df_nes.update(row)\n",
    "\n",
    "#rename cols in nes feature dataframe, merge to data as columns\n",
    "df_nes.columns = ['nes_cnt_'+nes for nes in all_nes]\n",
    "df = pd.merge(df, df_nes, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### parts of speech\n",
    "- NOTE: used temp hack to get this working on windows see [github issue](https://github.com/nltk/nltk/issues/1237) and [stackoverflow responses](http://stackoverflow.com/questions/34361725/nltk-stanfordnertagger-noclassdeffounderror-org-slf4j-loggerfactory-in-windo/34403741#34403741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanford_POS = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "#NEEDED - temp hack fix until nltk update\n",
    "stanford_dir = stanford_POS._stanford_jar.rpartition('\\\\')[0] #gets existing jar directory\n",
    "stanford_jars = find_jars_within_path(stanford_dir) #finds all jars in directory and subdirectories\n",
    "stanford_POS._stanford_jar = ';'.join(stanford_jars) #join paths add all jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default properties from tagger C:\\Users\\Kain\\stanford-postagger-full\\models\\english-bidirectional-distsim.tagger\r\n",
      "Reading POS tagger model from C:\\Users\\Kain\\stanford-postagger-full\\models\\english-bidirectional-distsim.tagger ... done [1.5 sec].\r\n",
      "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\r\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:125)\r\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:33)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.runTagInference(TestSentence.java:322)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.testTagInference(TestSentence.java:309)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.tagSentence(TestSentence.java:132)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagSentence(MaxentTagger.java:996)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagCoreLabelsOrHasWords(MaxentTagger.java:1787)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagAndOutputSentence(MaxentTagger.java:1797)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1708)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1769)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1542)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1498)\r\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1841)\r\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['C:\\\\Program Files\\\\Java\\\\jdk1.8.0_71\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger-3.6.0-javadoc.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger-3.6.0-sources.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger-3.6.0.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\lib\\\\slf4j-api.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\lib\\\\slf4j-simple.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\models\\\\english-bidirectional-distsim.tagger', '-textFile', 'C:\\\\Users\\\\Kain\\\\AppData\\\\Local\\\\Temp\\\\tmp3ha3q2fn', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-4063d97097f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"def getPOS(tweet):\\n    tokens = tknzr.tokenize(html.unescape(str(tweet)).replace('#', '').replace('@', ''))\\n    pos = stanford_POS.tag(tokens)\\n    return dict(pos)\\n\\ndf['parts_of_speech'] = df['text'].apply(lambda text: getPOS(text))\\ndf['count_parts_of_speech'] = df['parts_of_speech'].apply(lambda pos: len(pos))\\ndf['unique_parts_of_speech'] = df['parts_of_speech'].apply(lambda pos: len(set(pos.values())))\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2235\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2237\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2239\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas\\lib.c:63043)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(text)\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mgetPOS\u001b[1;34m(tweet)\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Run the tagger and get the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[1;32m---> 89\u001b[1;33m                                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mjava\u001b[1;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Java command failed : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Java command failed : ['C:\\\\Program Files\\\\Java\\\\jdk1.8.0_71\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger-3.6.0-javadoc.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger-3.6.0-sources.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger-3.6.0.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\stanford-postagger.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\lib\\\\slf4j-api.jar;C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\lib\\\\slf4j-simple.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'C:\\\\Users\\\\Kain\\\\stanford-postagger-full\\\\models\\\\english-bidirectional-distsim.tagger', '-textFile', 'C:\\\\Users\\\\Kain\\\\AppData\\\\Local\\\\Temp\\\\tmp3ha3q2fn', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def getPOS(tweet):\n",
    "    tokens = tknzr.tokenize(html.unescape(str(tweet)).replace('#', '').replace('@', ''))\n",
    "    pos = stanford_POS.tag(tokens)\n",
    "    return dict(pos)\n",
    "\n",
    "df['parts_of_speech'] = df['text'].apply(lambda text: getPOS(text))\n",
    "df['count_parts_of_speech'] = df['parts_of_speech'].apply(lambda pos: len(pos))\n",
    "df['unique_parts_of_speech'] = df['parts_of_speech'].apply(lambda pos: len(set(pos.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### expand/flatten parts of speech as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_pos = []\n",
    "for _,doc in df.iterrows():\n",
    "    all_pos.extend(list(doc['parts_of_speech'].values()))\n",
    "\n",
    "all_pos = list(set(all_pos))\n",
    "\n",
    "#initialize empty dataframe, twitter_id by pos type columns, filled with 0's\n",
    "df_pos = pd.DataFrame(data=0, index=df.index, columns=all_pos)\n",
    "\n",
    "for twitter_id,tweet in df.iterrows():\n",
    "    \n",
    "    #get pos type counts\n",
    "    pos_counts = dict(Counter(list(dict(tweet['parts_of_speech']).values())))\n",
    "    \n",
    "    #transform tweet pos type counts to uniform dataframe row\n",
    "    pos_dict = dict.fromkeys(all_pos)\n",
    "    for p in pos_counts.keys():\n",
    "        pos_dict[p] = pos_counts.get(p)\n",
    "    row = pd.Series(pos_dict, index=all_pos)\n",
    "    row = pd.DataFrame(row).T\n",
    "\n",
    "    #update pos feature dataframe\n",
    "    row['twitter_id'] = twitter_id\n",
    "    row.set_index('twitter_id', inplace=True)\n",
    "    df_pos.update(row)\n",
    "\n",
    "#rename cols in pos feature dataframe, merge to data as columns\n",
    "df_pos.columns = ['pos_cnt_'+pos for pos in all_pos]\n",
    "df = pd.merge(df, df_pos, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES: 67\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    " 'tweet_type',\n",
    " 'possibly_sensitive',\n",
    " 'favorite_count',\n",
    " 'retweet_count',\n",
    " 'user_default_profile',\n",
    " 'user_default_profile_image',\n",
    " 'user_verified',\n",
    " 'user_statuses',\n",
    " 'user_favourites',\n",
    " 'user_followers',\n",
    " 'user_friends',\n",
    " 'user_listed',\n",
    " 'source_web_client',\n",
    " 'source_tweetdeck',\n",
    " 'source_iphone',\n",
    " 'source_android',\n",
    " 'has_location',\n",
    " 'count_entities_media',\n",
    " 'media_per_word',\n",
    " 'count_entities_urls',\n",
    " 'urls_per_word',\n",
    " 'count_entities_mentions',\n",
    " 'mentions_per_word',\n",
    " 'count_entities_hashtags',\n",
    " 'hashtags_per_word',\n",
    " 'engagements',\n",
    " 'engagements_per_word',\n",
    " 'favorite_count_eventstdz',\n",
    " 'retweet_count_eventstdz',\n",
    " 'engagements_eventstdz',\n",
    " 'user_bio_len',\n",
    " 'user_reputation',\n",
    " 'user_age_days',\n",
    " 'user_follower_rate',\n",
    " 'count_characters',\n",
    " 'count_non_characters',\n",
    " 'count_upper',\n",
    " 'count_tokens',\n",
    " 'mean_token_length',\n",
    " 'mean_token_frequency',\n",
    " 'count_tokens_stopped',\n",
    " 'count_stops',\n",
    " 'count_question_marks',\n",
    " 'elongation',\n",
    " 'ellipsis',\n",
    " 'lexical_diversity',\n",
    " 'sfpp',\n",
    " 'pfpp',\n",
    " 'text_sentiment_polarity',\n",
    " 'text_sentiment_positive',\n",
    " 'text_sentiment_negative',\n",
    " 'text_sentiment_subjectivity',\n",
    " 'frequency_1min',\n",
    " 'rollmean_frequency_win5min',\n",
    " 'rollmean_frequency_deviation',\n",
    " 'media_weight',\n",
    " 'url_weight',\n",
    " 'mention_weight',\n",
    " 'hashtag_weight',\n",
    " 'term_weight',\n",
    " 'tfidf_mean',\n",
    " 'event_centroid_distance',\n",
    " 'count_named_entities',\n",
    " 'unique_named_entities',\n",
    " 'nes_cnt_LOCATION',\n",
    " 'nes_cnt_ORGANIZATION',\n",
    " 'nes_cnt_PERSON',\n",
    " #'count_parts_of_speech',\n",
    " #'unique_parts_of_speech',\n",
    " #'pos_cnt_TO',\n",
    " #'pos_cnt_RBR',\n",
    " #'pos_cnt_MD',\n",
    " #'pos_cnt_POS',\n",
    " #'pos_cnt_``',\n",
    " #'pos_cnt_,',\n",
    " #'pos_cnt_NNPS',\n",
    " #'pos_cnt_VBN',\n",
    " #'pos_cnt_PRP$',\n",
    " #'pos_cnt_WDT',\n",
    " #'pos_cnt_WP',\n",
    " #'pos_cnt_VBP',\n",
    " #'pos_cnt_$',\n",
    " #'pos_cnt_WRB',\n",
    " #\"pos_cnt_''\",\n",
    " #'pos_cnt_DT',\n",
    " #'pos_cnt_UH',\n",
    " #'pos_cnt_EX',\n",
    " #'pos_cnt_VBG',\n",
    " #'pos_cnt_JJ',\n",
    " #'pos_cnt_NNS',\n",
    " #'pos_cnt_VBZ',\n",
    " #'pos_cnt_:',\n",
    " #'pos_cnt_FW',\n",
    " #'pos_cnt_PRP',\n",
    " #'pos_cnt_CD',\n",
    " #'pos_cnt_VBD',\n",
    " #'pos_cnt_JJS',\n",
    " #'pos_cnt_SYM',\n",
    " #'pos_cnt_VB',\n",
    " #'pos_cnt_.',\n",
    " #'pos_cnt_PDT',\n",
    " #'pos_cnt_IN',\n",
    " #'pos_cnt_RB',\n",
    " #'pos_cnt_RP',\n",
    " #'pos_cnt_NN',\n",
    " #'pos_cnt_JJR',\n",
    " #'pos_cnt_NNP',\n",
    " #'pos_cnt_CC'\n",
    "]\n",
    "\n",
    "print('FEATURES: %s' % len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[x] = df[x].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[x] = df[x].replace(np.inf, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA F-Score\n",
    "- f-score\n",
    "- p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-aaa90697063e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'news'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mf_classif\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mf_regression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfeature\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mregression\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \"\"\"\n\u001b[1;32m--> 147\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msafe_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf_oneway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     52\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     53\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 54\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "fscores = sklearn.feature_selection.f_classif(df[x], df['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fscores = pd.DataFrame.from_records(fscores, index=['f-score', 'p-value'], columns=x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fscores[fscores['p-value'] < .05].sort_values(by='p-value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Squared\n",
    "- $\\chi^2$\n",
    "- p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-3542f2f031fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchi2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'news'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\miniconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "chi2 = sklearn.feature_selection.chi2(df[x], df['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi2 = pd.DataFrame.from_records(chi2, index=['x-square', 'p-value'], columns=x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chi2[chi2['p-value'] < .05].sort_values(by='p-value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using data from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def execute(clf, y):\n",
    "    print('_' * 40)\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    print('>>> ', clf_descr)\n",
    "    \n",
    "    t0 = time()\n",
    "    clf.fit(train[x], train[y])\n",
    "    train_time = time() - t0\n",
    "    print('  train time: %0.3fs' % train_time)\n",
    "    \n",
    "    t0 = time()\n",
    "    pred = clf.predict(test[x])\n",
    "    test_time = time() - t0\n",
    "    print('  test time:  %0.3fs' % test_time)\n",
    "    \n",
    "    print('_' * 40)\n",
    "    \n",
    "    print()\n",
    "    acc = metrics.accuracy_score(test[y], pred)\n",
    "    print('Accuracy: %0.3f' % acc)\n",
    "    auc = metrics.roc_auc_score(test[y], pred)\n",
    "    print('ROC AUC: %0.3f' % auc)\n",
    "    print()\n",
    "    \n",
    "    print('Confusion Matrix: \\n', pd.crosstab(test[y], pred, rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "    print()\n",
    "    print('Report: \\n', metrics.classification_report(test[y], pred))\n",
    "    \n",
    "    precision = metrics.precision_score(test[y], pred)\n",
    "    print('Precision: %0.3f' % precision)\n",
    "    recall = metrics.recall_score(test[y], pred)\n",
    "    print('Recall: %0.3f' % recall)\n",
    "    avg_precision = metrics.average_precision_score(test[y], pred, average='macro')\n",
    "    print('Class-Weighted AVG Precision: %0.3f' % avg_precision)\n",
    "    \n",
    "    top5 = []\n",
    "    if clf_descr in ['DecisionTreeClassifier', 'RandomForestClassifier']:\n",
    "        #top feature indices\n",
    "        k = 5\n",
    "        for i in np.argsort(clf.feature_importances_)[::-1][:k]:\n",
    "            top5.append([x[i], clf.feature_importances_[i]])\n",
    "        print('Top Features:')\n",
    "        print(top5)\n",
    "    \n",
    "    return y, clf_descr, acc, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "y = 'news'\n",
    "\n",
    "print('=' * 60)\n",
    "print(y.upper())\n",
    "print()\n",
    "data = df[df[y].isin([0,1])]\n",
    "train, test = train_test_split(data, test_size=.2, random_state=2016) \n",
    "print('Class Distribution:')\n",
    "\n",
    "for d,name in ((data, 'Total'), (train, 'Train'), (test, 'Test')):\n",
    "    t = len(d)\n",
    "    n = len(d[d[y]==0])\n",
    "    s = len(d[d[y]==1])\n",
    "    print(name, '\\t', '0: %4.0f (%0.2f%%)' % (n, (n/t)), '  ', '1: %4.0f (%0.2f%%)' % (s, (s/t)))\n",
    "\n",
    "print()\n",
    "print('=' * 60)\n",
    "    \n",
    "for clf in (#MultinomialNB(), \n",
    "            #KNeighborsClassifier(),\n",
    "            DecisionTreeClassifier(random_state=2016),\n",
    "            RandomForestClassifier(random_state=2016),\n",
    "            svm.SVC()\n",
    "            ):\n",
    "    results.append(execute(clf, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.columns = ['y', 'classifier', 'accuracy', 'precision', 'recall']\n",
    "\n",
    "for name, result in results.groupby('y'):\n",
    "    print(name)\n",
    "    result.plot(x='classifier', kind='bar', ylim=[0,1], title='Classification Using Custom Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute(clf, y):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    print('_' * 40)\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    print('>>> ', clf_descr)\n",
    "    \n",
    "    trainBOW = vectorizer.fit_transform(train['text'])\n",
    "    t0 = time()\n",
    "    clf.fit(trainBOW, train[y])\n",
    "    train_time = time() - t0\n",
    "    print('  train time: %0.3fs' % train_time)\n",
    "    \n",
    "    testBOW = vectorizer.transform(test['text'])\n",
    "    t0 = time()\n",
    "    pred = clf.predict(testBOW)\n",
    "    test_time = time() - t0\n",
    "    print('  test time:  %0.3fs' % test_time)\n",
    "    \n",
    "    print('_' * 40)\n",
    "    \n",
    "    print()\n",
    "    acc = metrics.accuracy_score(test[y], pred)\n",
    "    print('Accuracy: %0.3f' % acc)\n",
    "    auc = metrics.roc_auc_score(test[y], pred)\n",
    "    print('ROC AUC: %0.3f' % auc)\n",
    "    print()\n",
    "    \n",
    "    print('Confusion Matrix: \\n', pd.crosstab(test[y], pred, rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "    print()\n",
    "    print('Report: \\n', metrics.classification_report(test[y], pred))\n",
    "    \n",
    "    precision = metrics.precision_score(test[y], pred)\n",
    "    print('Precision: %0.3f' % precision)\n",
    "    recall = metrics.recall_score(test[y], pred)\n",
    "    print('Recall: %0.3f' % recall)\n",
    "    avg_precision = metrics.average_precision_score(test[y], pred, average='macro')\n",
    "    print('Class-Weighted AVG Precision: %0.3f' % avg_precision)\n",
    "    \n",
    "    top5 = []\n",
    "    if clf_descr in ['DecisionTreeClassifier', 'RandomForestClassifier']:\n",
    "        #top feature indices\n",
    "        k = 5\n",
    "        for i in np.argsort(clf.feature_importances_)[::-1][:k]:\n",
    "            top5.append([vectorizer.get_feature_names()[i], clf.feature_importances_[i]])\n",
    "        print('Top Features:')\n",
    "        print(top5)\n",
    "    \n",
    "    return y, clf_descr, acc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "y = 'news'\n",
    "\n",
    "\n",
    "print('=' * 60)\n",
    "print(y.upper())\n",
    "print()\n",
    "#data = df[(df['event'] == 'EgyptAir Flight 804') & (df[y].isin([0,1]))]\n",
    "data = df\n",
    "train, test = train_test_split(data, test_size=.2, random_state=2016) \n",
    "print('Class Distribution:')\n",
    "for d,name in ((data, 'Total'), (train, 'Train'), (test, 'Test')):\n",
    "    t = len(d)\n",
    "    n = len(d[d[y]==0])\n",
    "    s = len(d[d[y]==1])\n",
    "    print(name, '\\t', '0: %4.0f (%0.2f%%)' % (n, (n/t)), '  ', '1: %4.0f (%0.2f%%)' % (s, (s/t)))\n",
    "\n",
    "print()\n",
    "print('=' * 60)\n",
    "    \n",
    "for clf in (#MultinomialNB(), \n",
    "            KNeighborsClassifier(),\n",
    "            DecisionTreeClassifier(random_state=2016),\n",
    "            RandomForestClassifier(random_state=2016),\n",
    "            svm.SVC()\n",
    "            ):\n",
    "    results.append(execute(clf, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.columns = ['y', 'classifier', 'accuracy', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, result in results.groupby('y'):\n",
    "    print(name)\n",
    "    result.plot(x='classifier', kind='bar', ylim=[0,1], title='Classification Using TFIDF Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis of bag of words\n",
    "\n",
    "We learn that when training a classifier using a bag of words approach on one specific event, we can predict the news in that event to a high degree, and even with a precision up to 1. This is why this approach is so widely seen in literature. \n",
    "\n",
    "Although most research stops at their successful results and does not recognize how their findings may apply to the real world. The intentions of training a classifier on text is to be able to classify text which is unseen. The reasons the classifier did so well is due to the text in the testing corpus being so similar to that in the training corpus. The data was trained on a single event and test on the same event.\n",
    "\n",
    "If we are apply this classifier to any different event the accuracy decreases significantly, to levels below the results of our custom features even. Similarily if we train the bag of words classifier using multiple events from different domains the results are also lower than our custom results. This shows how using the bag of words model, is not useful to predict anything outside of the domain it was classified in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENT: greedy filter noise, for annotating more news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('total:')\n",
    "n = df.shape[0]\n",
    "k = df[df['y_news']==1].shape[0]\n",
    "print('%.2f%% (%s docs) of %s data marked with news:' % ((k/n*100), k, n))\n",
    "d = df\n",
    "#question\n",
    "print('question:')\n",
    "d = d[d['question'] == 0]\n",
    "n = d.shape[0]\n",
    "k = d[d['y_news']==1].shape[0]\n",
    "print('%.2f%% (%s docs) of %s data marked with news:' % ((k/n*100), k, n))\n",
    "#first person\n",
    "print('first person:')\n",
    "d = d[d['sfpp'] == 0]\n",
    "n = d.shape[0]\n",
    "k = d[d['y_news']==1].shape[0]\n",
    "print('%.2f%% (%s docs) of %s data marked with news:' % ((k/n*100), k, n))\n",
    "#sentiment negative\n",
    "print('sentiment negative:')\n",
    "d = d[d['text_sentiment_negative'] == 0]\n",
    "n = d.shape[0]\n",
    "k = d[d['y_news']==1].shape[0]\n",
    "print('%.2f%% (%s docs) of %s data marked with news:' % ((k/n*100), k, n))\n",
    "#sentiment positive\n",
    "print('sentiment positive:')\n",
    "d = d[d['text_sentiment_positive'] == 0]\n",
    "n = d.shape[0]\n",
    "k = d[d['y_news']==1].shape[0]\n",
    "print('%.2f%% (%s docs) of %s data marked with news:' % ((k/n*100), k, n))\n",
    "#subjectivity\n",
    "print('sentiment subjective:')\n",
    "d = d[d['text_sentiment_subjective'] == 0]\n",
    "n = d.shape[0]\n",
    "k = d[d['y_news']==1].shape[0]\n",
    "print('%.2f%% (%s docs) of %s data marked with news:' % ((k/n*100), k, n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
