{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "event_name = '[TEDxNations]'\n",
    "event_data = pd.read_table('data/%s_data.txt' % event_name, sep='\\t', header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove links\n",
    "event_data['text_nolink'] = event_data['text'].apply(lambda text: ' '.join([(w[:w.find('http')] if 'http' in w else w) for w in text.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show duplicates on raw text *without link (uncomment next line)\n",
    "#event_data[event_data.duplicated(['text_nolink'], take_last=True) | event_data.duplicated(['text_nolink'])].sort('text')\n",
    "#drop duplicates\n",
    "event_data = event_data.drop_duplicates('text_nolink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove hashtags\n",
    "#only remove symbol\n",
    "event_data['text_clean'] = event_data['text_nolink'].apply(lambda text: text.replace('#', ''))\n",
    "#remove entire token\n",
    "#event_data['text_clean'] = event_data['text_clean'].apply(lambda text: ' '.join([w for w in text.split() if not w.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove mentions\n",
    "#only remove symbol\n",
    "event_data['text_clean'] = event_data['text_clean'].apply(lambda text: text.replace('@', ''))\n",
    "#remove entire token\n",
    "#event_data['text_clean'] = event_data['text_clean'].apply(lambda text: ' '.join([w for w in text.split() if not w.startswith('@')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove ampersand\n",
    "event_data['text_clean'] = event_data['text_clean'].apply(lambda text: text.replace('&amp;', ' & '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize, remove stopwords\n",
    "#method preserves mentions and hashtags\n",
    "tknzr = TweetTokenizer()\n",
    "#alternatives\n",
    "#nltk.word_tokenize()\n",
    "#nltk.tokenize.wordpunct_tokenize()\n",
    "#tknzr_regrex.tokenize()\n",
    "#create stop word list\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "punct = list(string.punctuation)\n",
    "punct.extend(['...', '..', '…', '”', '“', 'the', '.@', 'RT'])\n",
    "stop.extend(punct)\n",
    "#apply tokenization, stopword removal\n",
    "event_data['text_clean_tokens'] = event_data['text_clean'].apply(lambda s: ' '.join([w.lower() for w in tknzr.tokenize(str(s)) if w.lower() not in stop and len(w) > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stemming\n",
    "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "#alternatives\n",
    "#stemmer = PorterStemmer()\n",
    "#apply stemming\n",
    "event_data['text_clean_stems'] = event_data['text_clean_tokens'].apply(lambda tokens: ' '.join([stemmer.stem(str(s)) for s in tokens.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: \"How does @ICRC work globally to address #sexualviolence in conflict? Learn more here: https://t.co/So5vbZAxPw  #TEDxNations via @PMeigeICRC\"\n",
      "no_link: \"How does @ICRC work globally to address #sexualviolence in conflict? Learn more here:  #TEDxNations via @PMeigeICRC\"\n",
      "clean: \"How does ICRC work globally to address sexualviolence in conflict? Learn more here:  TEDxNations via PMeigeICRC\"\n",
      "tokens: \"icrc work globally address sexualviolence conflict learn tedxnations via pmeigeicrc\"\n",
      "stems: \"icrc work global address sexualviol conflict learn tedxnat via pmeigeicrc\"\n"
     ]
    }
   ],
   "source": [
    "#TEST EXAMPLE Cleaning\n",
    "tweet = event_data.iloc[0]\n",
    "print('raw: \"%s\"' % tweet['text'])\n",
    "print('no_link: \"%s\"' % tweet['text_nolink'])\n",
    "print('clean: \"%s\"' % tweet['text_clean'])\n",
    "print('tokens: \"%s\"' % tweet['text_clean_tokens'])\n",
    "print('stems: \"%s\"' % tweet['text_clean_stems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doc clean columns\n",
    "event_data_clean = event_data[['id', 'created_at', 'text', 'text_nolink', 'text_clean', 'text_clean_tokens', 'text_clean_stems']]\n",
    "#save clean data\n",
    "event_data_clean.to_csv('data/%s_data_clean.txt' % event_name, sep='\\t', encoding='utf-8', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
