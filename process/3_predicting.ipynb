{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import nltk\n",
    "import html\n",
    "import string\n",
    "import ast\n",
    "from textblob import TextBlob\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Event Name: Xbox E3\n"
     ]
    }
   ],
   "source": [
    "event_name = input('Enter Event Name: ')\n",
    "event_filename = re.sub(\"\\W+\", \"\", event_name.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86475, 36)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/final/event_%s_data.txt' % event_filename, sep='\\t', encoding='utf-8', header=0, parse_dates=['created_at'], dtype={'twitter_id' : 'str'})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 5)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsubevents = pd.read_csv('data/final/event_%s_subevents.txt' % event_filename, sep='\\t', encoding='utf-8', header=0, index_col=0)\n",
    "dfsubevents.index = dfsubevents.index.to_datetime()\n",
    "dfsubevents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 7)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e_subevents = pd.read_csv('data/final/event_%s_e_subevents.txt' % event_filename, sep='\\t', encoding='utf-8', header=0, index_col=0)\n",
    "df_e_subevents.index = df_e_subevents.index.to_datetime()\n",
    "df_e_subevents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allsubevents = pd.merge(dfsubevents[['count', 'mean', 'deviation', 'rank_subevents']], df_e_subevents[['count+', 'rank_subevents']], how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "dftemp = df[['twitter_id', 'created_at']]\n",
    "dftemp['created_at'] = dftemp['created_at'].apply(lambda x: x.replace(second=0))\n",
    "dftemp.set_index(['created_at'], inplace=True)\n",
    "\n",
    "data = pd.merge(dftemp, allsubevents, how='inner', left_index=True, right_index=True)\n",
    "data.set_index('twitter_id', inplace=True)\n",
    "\n",
    "data = pd.merge(df.set_index('twitter_id'), data, how='left', left_index=True, right_index=True)\n",
    "\n",
    "data = data[(data['rank_subevents_x'] <= k) | (data['rank_subevents_y'] <= k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25069, 41)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text'].apply(lambda text: ' '.join([(w[:w.find('http')] if 'http' in w else w) for w in html.unescape(str(text)).replace('#', '').replace('@', '').split()]))\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "data['text_clean'] = data['text_clean'].apply(lambda text: ' '.join([w.lower() for w in tknzr.tokenize(text)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['source_web_client'] = data['tweet_type'].apply(lambda s: [0,1][s=='Twitter Web Client'])\n",
    "data['source_tweetdeck'] = data['tweet_type'].apply(lambda s: [0,1][s=='TweetDeck'])\n",
    "data['source_iphone'] = data['tweet_type'].apply(lambda s: [0,1][s=='Twitter for iPhone'])\n",
    "data['source_android'] = data['tweet_type'].apply(lambda s: [0,1][s=='Twitter for Android'])\n",
    "data['has_location'] = data['longitude'].apply(lambda l: [1,0][np.isnan(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['tweet_type'] = data['tweet_type'].apply(lambda d: [0,1][d=='media'])\n",
    "data['possibly_sensitive'] = data['possibly_sensitive'].apply(lambda d: [0,1][d==True])\n",
    "data['count_entities_media'] = data['entities_media'].apply(lambda media: len(media))\n",
    "data['media_per_word'] = data.apply(lambda row: row['count_entities_media'] / len(row['text'].split()), axis=1)\n",
    "data['count_entities_urls'] = data['entities_urls'].apply(lambda urls: len(urls))\n",
    "data['urls_per_word'] = data.apply(lambda row: row['count_entities_urls'] / len(row['text'].split()), axis=1)\n",
    "data['count_entities_mentions'] = data['entities_mentions'].apply(lambda mentions: len(mentions))\n",
    "data['mentions_per_word'] = data.apply(lambda row: row['count_entities_mentions'] / len(row['text'].split()), axis=1)\n",
    "data['count_entities_hashtags'] = data['entities_hashtags'].apply(lambda tags: len(tags))\n",
    "data['hashtags_per_word'] = data.apply(lambda row: row['count_entities_hashtags'] / len(row['text'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['engagements'] = data['favorite_count'] + data['retweet_count']\n",
    "data['engagements_per_word'] = data.apply(lambda row: row['engagements'] / len(row['text'].split()), axis=1)\n",
    "data['favorite_count'] = data.groupby('event')['favorite_count'].transform(lambda x: x / x.max())\n",
    "data['retweet_count'] = data.groupby('event')['retweet_count'].transform(lambda x: x / x.max())\n",
    "data['engagements'] = data.groupby('event')['engagements'].transform(lambda x: x / x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['user_verified'] = data['user_verified'].apply(lambda d: [0,1][d==True])\n",
    "data['user_default_profile'] = data['user_default_profile'].apply(lambda d: [0,1][d==True])\n",
    "data['user_default_profile_image'] = data['user_default_profile_image'].apply(lambda d: [0,1][d==True])\n",
    "data['user_reputation'] = data['user_followers'] / (data['user_friends'])\n",
    "data['user_reputation'].replace(np.inf, np.nan, inplace=True)\n",
    "data['user_bio_len'] = data['user_description'].apply(lambda bio: len(str(bio)))\n",
    "data['user_age_days'] = data.apply(lambda doc: (doc['created_at'] - pd.to_datetime(doc['user_created_at'])).days, axis=1)\n",
    "data['user_follower_rate'] = data['user_followers'] / data['user_age_days']\n",
    "data['user_follower_rate'].replace(np.inf, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = nltk.corpus.stopwords.words('english')\n",
    "punct = list(string.punctuation)\n",
    "punct.extend(['...', '..', '…', '”', '“', '.@', 'RT'])\n",
    "stop.extend(punct)\n",
    "elongation = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n",
    "data['count_characters'] = data['text'].apply(lambda text: len(str(text)))\n",
    "data['count_non_characters'] = data['text'].apply(lambda text: len(re.sub('[\\w+!@#$%&;:,.?\\/\\-“”’`\"\\'()|]', '', text).strip()))\n",
    "data['count_upper'] = data['text'].apply(lambda text: len([l for l in ' '.join([w for w in text.split() if not w.startswith(('#', '@'))]) if l.isupper()]))\n",
    "data['count_tokens'] = data['text_clean'].apply(lambda text: len(text.split()))\n",
    "data['mean_token_length'] = data['text_clean'].apply(lambda text: np.mean([len(t) for t in text.split()]))\n",
    "data['mean_token_frequency'] = data['text_clean'].apply(lambda text: np.mean(list(Counter(text.split()).values())))\n",
    "data['count_tokens_stopped'] = data['text_clean'].apply(lambda text: len([t for t in text.split() if t not in stop]))\n",
    "data['count_stops'] = data['text_clean'].apply(lambda text: len([t for t in text.split() if t in stop]))\n",
    "data['count_question_marks'] = data['text_clean'].apply(lambda text: text.split().count('?'))\n",
    "data['elongation'] = data['text_clean'].apply(lambda text: [0,1][bool(elongation.search(text))])\n",
    "data['ellipsis'] = data['text_clean'].apply(lambda text: 1 if any(x in text for x in ('...', '…')) else 0)\n",
    "data['lexical_diversity'] = data['text_clean'].apply(lambda text: len(set(text.split())) / len(text.split()))\n",
    "sfpp = ['i', 'i\\'m', 'me', 'mine', 'my', 'myself']\n",
    "data['sfpp'] = data['text_clean'].apply(lambda text: 1 if any(t.lower() in sfpp for t in text.split()) else 0)\n",
    "pfpp = ['we', 'we\\'re', 'ours', 'our', 'ourselves']\n",
    "data['pfpp'] = data['text_clean'].apply(lambda text: 1 if any(t.lower() in pfpp for t in text.split()) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['text_sentiment_polarity'] = data['text_clean'].apply(lambda text: TextBlob(str(text)).sentiment.polarity)\n",
    "data['text_sentiment_positive'] = data['text_sentiment_polarity'].apply(lambda s: [0,abs(s)][s > 0])\n",
    "data['text_sentiment_negative'] = data['text_sentiment_polarity'].apply(lambda s: [0,abs(s)][s < 0])\n",
    "data['text_sentiment_subjectivity'] = data['text_clean'].apply(lambda text: TextBlob(str(text)).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.rename(columns={'count' : 'frequency_1min',\n",
    "                     'mean' : 'rollmean_frequency_win5min',\n",
    "                     'deviation' : 'rollmean_frequency_deviation'},\n",
    "         inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['media_weight'] = np.nan\n",
    "data['url_weight'] = np.nan\n",
    "data['mention_weight'] = np.nan\n",
    "data['hashtag_weight'] = np.nan\n",
    "data['term_weight'] = np.nan\n",
    "data['tfidf_mean'] = np.nan\n",
    "data['event_centroid_distance'] = np.nan\n",
    "\n",
    "dfevent = df[['twitter_id', 'text', 'entities_media', 'entities_urls', 'entities_mentions', 'entities_hashtags']]\n",
    "\n",
    "#clean text\n",
    "dfevent['text_clean'] = dfevent['text'].apply(lambda text: ' '.join([(w[:w.find('http')] if 'http' in w else w) for w in html.unescape(str(text)).replace('#', '').replace('@', '').split()]))\n",
    "dfevent['text_clean'] = dfevent['text_clean'].apply(lambda text: ' '.join([w.lower() for w in tknzr.tokenize(text)]))\n",
    "\n",
    "#media weight\n",
    "entities_lists = dfevent['entities_media'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "entity_counts = Counter(allentities)\n",
    "dfevent['media_weight'] = dfevent['entities_media'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "dfevent['media_weight'] = dfevent['media_weight'] / dfevent['media_weight'].max()\n",
    "#dfevent['media_weight'] = (dfevent['media_weight'] - dfevent['media_weight'].mean()) / dfevent['media_weight'].std()\n",
    "\n",
    "#url weight\n",
    "entities_lists = dfevent['entities_urls'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "entity_counts = Counter(allentities)\n",
    "dfevent['url_weight'] = dfevent['entities_urls'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "dfevent['url_weight'] = dfevent['url_weight'] / dfevent['url_weight'].max()\n",
    "#dfevent['url_weight'] = (dfevent['url_weight'] - dfevent['url_weight'].mean()) / dfevent['url_weight'].std()\n",
    "\n",
    "#mention weight\n",
    "entities_lists = dfevent['entities_mentions'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "entity_counts = Counter(allentities)\n",
    "dfevent['mention_weight'] = dfevent['entities_mentions'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "dfevent['mention_weight'] = dfevent['mention_weight'] / dfevent['mention_weight'].max()\n",
    "#dfevent['mention_weight'] = (dfevent['mention_weight'] - dfevent['mention_weight'].mean()) / dfevent['mention_weight'].std()\n",
    "\n",
    "#hashtag weight\n",
    "entities_lists = dfevent['entities_hashtags'].apply(lambda entities: ast.literal_eval(entities)).values\n",
    "allentities = [e.lower() for elist in entities_lists for e in elist]\n",
    "entity_counts = Counter(allentities)\n",
    "dfevent['hashtag_weight'] = dfevent['entities_hashtags'].apply(lambda entities: sum([entity_counts.get(e.lower()) for e in ast.literal_eval(entities)]))\n",
    "dfevent['hashtag_weight'] = dfevent['hashtag_weight'] / dfevent['mention_weight'].max()\n",
    "#dfevent['hashtag_weight'] = (dfevent['hashtag_weight'] - dfevent['hashtag_weight'].mean()) / dfevent['mention_weight'].std()\n",
    "\n",
    "#term weight\n",
    "tweets = [str(d) for d in dfevent['text_clean']]\n",
    "tokens = [w for t in tweets for w in t.split() if w not in stop] \n",
    "token_counts = Counter(tokens)\n",
    "dfevent['term_weight'] = dfevent['text_clean'].apply(lambda text: sum([token_counts.get(t) for t in text.split() if t not in stop]))\n",
    "dfevent['term_weight'] = dfevent['term_weight'] / dfevent['term_weight'].max()\n",
    "#dfevent['term_weight'] = (dfevent['term_weight'] - dfevent['term_weight'].mean()) / dfevent['term_weight'].std()\n",
    "    \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "xtfidf = tfidf_vectorizer.fit_transform(dfevent['text_clean'])\n",
    "\n",
    "#tfidf mean\n",
    "xtfidf_means = xtfidf.mean(axis=1)\n",
    "xtfidf_means = pd.DataFrame(xtfidf_means, columns=['tfidf_mean'])\n",
    "xtfidf_means['tfidf_mean'] = xtfidf_means['tfidf_mean'] / xtfidf_means['tfidf_mean'].max()\n",
    "dfevent = pd.merge(dfevent, xtfidf_means, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "#tfidf centroid distance\n",
    "xtfidf_centroid = xtfidf.mean(axis=0)\n",
    "xtfidf_cosdistance = pairwise_distances(X=xtfidf, Y=xtfidf_centroid, metric='cosine')\n",
    "xtfidf_cosdistance = pd.DataFrame(xtfidf_cosdistance, columns=['event_centroid_distance'])\n",
    "xtfidf_cosdistance['event_centroid_distance'] = xtfidf_cosdistance['event_centroid_distance'] / xtfidf_cosdistance['event_centroid_distance'].max()\n",
    "dfevent = pd.merge(dfevent, xtfidf_cosdistance, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "#set index back to twitter id\n",
    "dfevent.set_index('twitter_id', inplace=True)\n",
    "\n",
    "#update columns in dataframe, on index (twitter_id)\n",
    "data.update(dfevent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES: 58\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    " 'tweet_type',\n",
    " 'possibly_sensitive',\n",
    " 'user_default_profile',\n",
    " 'user_default_profile_image',\n",
    " 'user_verified',\n",
    " 'user_statuses',\n",
    " 'user_favourites',\n",
    " 'user_followers',\n",
    " 'user_friends',\n",
    " 'user_listed',\n",
    " 'source_web_client',\n",
    " 'source_tweetdeck',\n",
    " 'source_iphone',\n",
    " 'source_android',\n",
    " 'has_location',\n",
    " 'count_entities_media',\n",
    " 'media_per_word',\n",
    " 'count_entities_urls',\n",
    " 'urls_per_word',\n",
    " 'count_entities_mentions',\n",
    " 'mentions_per_word',\n",
    " 'count_entities_hashtags',\n",
    " 'hashtags_per_word',\n",
    " 'favorite_count',\n",
    " 'retweet_count',\n",
    " 'engagements',\n",
    " 'engagements_per_word',\n",
    " 'user_bio_len',\n",
    " 'user_reputation',\n",
    " 'user_age_days',\n",
    " 'user_follower_rate',\n",
    " 'count_characters',\n",
    " 'count_non_characters',\n",
    " 'count_upper',\n",
    " 'count_tokens',\n",
    " 'mean_token_length',\n",
    " 'mean_token_frequency',\n",
    " 'count_tokens_stopped',\n",
    " 'count_stops',\n",
    " 'count_question_marks',\n",
    " 'elongation',\n",
    " 'ellipsis',\n",
    " 'lexical_diversity',\n",
    " 'sfpp',\n",
    " 'pfpp',\n",
    " 'text_sentiment_positive',\n",
    " 'text_sentiment_negative',\n",
    " 'text_sentiment_subjectivity',\n",
    " 'frequency_1min',\n",
    " 'rollmean_frequency_win5min',\n",
    " 'rollmean_frequency_deviation',\n",
    " 'media_weight',\n",
    " 'url_weight',\n",
    " 'mention_weight',\n",
    " 'hashtag_weight',\n",
    " 'term_weight',\n",
    " 'tfidf_mean',\n",
    " 'event_centroid_distance'\n",
    "]\n",
    "\n",
    "print('FEATURES: %s' % len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[x] = data[x].apply(lambda x: x / x.max(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[x] = data[x].fillna(0)\n",
    "data[x] = data[x].replace(np.inf, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('clf/clfCUSTOM_train_Mix_test_Mix_RandomForestClassifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kbestfeatures = pd.read_csv('clf/kbestfeatures_CUSTOM_train_Mix_test_Mix_RandomForestClassifier.txt', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xkbest = list(kbestfeatures['feature'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['news'] = clf.predict(data[xkbest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set news as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['news'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update df with classified news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.set_index('twitter_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.update(data['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86475, 36)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_filename = re.sub(\"\\W+\", \"\", event_name.strip())\n",
    "df.to_csv('data/final/event_%s_data.txt' % event_filename, sep='\\t', encoding='utf-8', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
