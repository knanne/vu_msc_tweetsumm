{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TERM FREQUENCY - tf sub(t,d) = sum of occurences term (t) in doc (d)\n",
    "#DOCUMENT FREQUENCY - df sub(t) = sum of docs (d) in collection containing term (t)\n",
    "#INVERSE DOCUMENT FREQUENCY - idf sub(t) = log of total docs in collection (N) over document frequency\n",
    "#WEIGHTED TF-IDF - tf-idf sub(t,d) - sum of occurences term (t) in doc (d) times inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elongated = re.compile('([a-zA-Z])\\\\1{2,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_name = '[egyptair]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "event_data = pd.read_table('data/%s_data_clean.txt' % event_name, sep='\\t', header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'created_at',\n",
       " 'text',\n",
       " 'text_nolink',\n",
       " 'text_clean',\n",
       " 'text_clean_tokens',\n",
       " 'text_clean_stems']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(event_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add basic count features\n",
    "event_data['count_links'] = event_data['text'].apply(lambda text: len([w for w in text.split() if w.startswith(('http://', 'https://'))]))\n",
    "event_data['count_hashtags'] = event_data['text'].apply(lambda text: len([w for w in text.split() if w.startswith('#')]))\n",
    "event_data['count_mentions'] = event_data['text'].apply(lambda text: len([w for w in text.split() if w.startswith('@')]))\n",
    "event_data['count_words'] = event_data['text'].apply(lambda text: len([w for w in text.split() if not w.startswith(('RT', '@', '#'))]))\n",
    "event_data['count_stopwords'] = event_data.apply(lambda doc: len(doc['text_clean'].split()) - len(doc['text_clean_tokens'].split()), axis=1)\n",
    "event_data['count_characters'] = event_data['text'].apply(lambda text: len(str(text)))\n",
    "event_data['count_non_characters'] = event_data['text_nolink'].apply(lambda text: len(re.sub('[\\w+!@#$%&;:,.?\\/\\-“”’`\"\\'()|]', '', text).strip()))\n",
    "event_data['count_upper'] = event_data['text_nolink'].apply(lambda text: len([l for l in ' '.join([w for w in text.split() if not w.startswith(('#', '@'))]) if l.isupper()]))\n",
    "event_data['bool_question'] = event_data['text_clean'].apply(lambda text: 1 if '?' in text else 0)\n",
    "event_data['bool_elongation'] = event_data['text_clean'].apply(lambda text: 1 if bool(elongated.search(text)) else 0)\n",
    "event_data['bool_ellipsis'] = event_data['text_clean'].apply(lambda text: 1 if any(x in text for x in ('...', '…')) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute top tokens\n",
    "def gather_tokens(data):\n",
    "    all_tokens = []\n",
    "    for doc in data:\n",
    "        tokens = doc.split()\n",
    "        all_tokens.extend(tokens)\n",
    "    return all_tokens\n",
    "\n",
    "#gather tokens from all docs \n",
    "all_tokens = gather_tokens(event_data['text_clean_tokens'])\n",
    "#create counter object\n",
    "tokens_cntr = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('egyptair', 5183),\n",
       " ('plane', 1550),\n",
       " ('hijacked', 1531),\n",
       " ('hijacker', 1253),\n",
       " ('cyprus', 1148),\n",
       " ('passengers', 779),\n",
       " ('flight', 618),\n",
       " ('hijacking', 599),\n",
       " ('hijack', 571),\n",
       " ('love', 535)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_cntr.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize a vectorizer, require minimum freq. of terms at 2\n",
    "count_vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Learn the vocabulary dictionary and return term-document matrix\n",
    "train_matrix_cnt = count_vect.fit_transform(event_data['text_clean_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit and Transform count sparse matrix to normalized tf-idf sparse matrix\n",
    "#first fit transformer which computes idf values\n",
    "tfidf_transformer = TfidfTransformer().fit(train_matrix_cnt)\n",
    "#second transform back to sparse matrix with tfidf values\n",
    "train_matrix_tfidf = tfidf_transformer.transform(train_matrix_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (5297, 2371)\n",
      "size: 12559187\n",
      "non-zeros: 46425\n",
      "sparsity: 99.63%\n",
      "density: 0.37%\n"
     ]
    }
   ],
   "source": [
    "#explore sparse matrix\n",
    "print('sparse matrix shape:', train_matrix_cnt.shape)\n",
    "print('size:', (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]))\n",
    "print('non-zeros:', train_matrix_cnt.getnnz())\n",
    "print('sparsity: %.2f%%' % (100.0 * (((train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]) - train_matrix_cnt.getnnz()) / (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]))))\n",
    "print('density: %.2f%%' % (100.0 * train_matrix_cnt.getnnz() / (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>tf</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foreign</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>5.7684</td>\n",
       "      <td>0.2694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ministry</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>5.1324</td>\n",
       "      <td>0.2397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>denies</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.4956</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>told</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.4956</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guardian</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>6.8025</td>\n",
       "      <td>0.6353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>egyptair</td>\n",
       "      <td>1</td>\n",
       "      <td>5183</td>\n",
       "      <td>1.0434</td>\n",
       "      <td>0.0487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hijacker</td>\n",
       "      <td>1</td>\n",
       "      <td>1253</td>\n",
       "      <td>2.4734</td>\n",
       "      <td>0.1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>idiot</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>4.7388</td>\n",
       "      <td>0.2213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spoke</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8.4765</td>\n",
       "      <td>0.3958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term tf    df     idf   tfidf\n",
       "0   foreign  1    44  5.7684  0.2694\n",
       "1  ministry  1    85  5.1324  0.2397\n",
       "2    denies  1     7  7.4956    0.35\n",
       "3      told  1     7  7.4956    0.35\n",
       "4  guardian  2    16  6.8025  0.6353\n",
       "5  egyptair  1  5183  1.0434  0.0487\n",
       "6  hijacker  1  1253  2.4734  0.1155\n",
       "7     idiot  1   130  4.7388  0.2213\n",
       "8     spoke  1     2  8.4765  0.3958"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST EXAMPLE term frequencies\n",
    "data_index = 0\n",
    "\n",
    "data = pd.DataFrame(columns=['term', 'tf', 'df', 'idf', 'tfidf'])\n",
    "\n",
    "for feature_index in train_matrix_cnt[data_index].nonzero()[1]:\n",
    "    \n",
    "    term = count_vect.get_feature_names()[feature_index]\n",
    "    tf = train_matrix_cnt[data_index, feature_index]\n",
    "    df = tokens_cntr.get(term)\n",
    "    idf = round(tfidf_transformer.idf_[feature_index], 4)\n",
    "    tfidf = round(train_matrix_tfidf[data_index, feature_index], 4)\n",
    "    \n",
    "    row = [term, tf, df, idf, tfidf]\n",
    "    row = pd.Series(row, index=['term', 'tf', 'df', 'idf', 'tfidf'])\n",
    "    row = pd.DataFrame(row).T\n",
    "    data = data.append(row, ignore_index=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete\n",
      "0.94% complete\n",
      "1.89% complete\n",
      "2.83% complete\n",
      "3.78% complete\n",
      "4.72% complete\n",
      "5.66% complete\n",
      "6.61% complete\n",
      "7.55% complete\n",
      "8.5% complete\n",
      "9.44% complete\n",
      "10.38% complete\n",
      "11.33% complete\n",
      "12.27% complete\n",
      "13.22% complete\n",
      "14.16% complete\n",
      "15.1% complete\n",
      "16.05% complete\n",
      "16.99% complete\n",
      "17.93% complete\n",
      "18.88% complete\n",
      "19.82% complete\n",
      "20.77% complete\n",
      "21.71% complete\n",
      "22.65% complete\n",
      "23.6% complete\n",
      "24.54% complete\n",
      "25.49% complete\n",
      "26.43% complete\n",
      "27.37% complete\n",
      "28.32% complete\n",
      "29.26% complete\n",
      "30.21% complete\n",
      "31.15% complete\n",
      "32.09% complete\n",
      "33.04% complete\n",
      "33.98% complete\n",
      "34.93% complete\n",
      "35.87% complete\n",
      "36.81% complete\n",
      "37.76% complete\n",
      "38.7% complete\n",
      "39.65% complete\n",
      "40.59% complete\n",
      "41.53% complete\n",
      "42.48% complete\n",
      "43.42% complete\n",
      "44.36% complete\n",
      "45.31% complete\n",
      "46.25% complete\n",
      "47.2% complete\n",
      "48.14% complete\n",
      "49.08% complete\n",
      "50.03% complete\n",
      "50.97% complete\n",
      "51.92% complete\n",
      "52.86% complete\n",
      "53.8% complete\n",
      "54.75% complete\n",
      "55.69% complete\n",
      "56.64% complete\n",
      "57.58% complete\n",
      "58.52% complete\n",
      "59.47% complete\n",
      "60.41% complete\n",
      "61.36% complete\n",
      "62.3% complete\n",
      "63.24% complete\n",
      "64.19% complete\n",
      "65.13% complete\n",
      "66.08% complete\n",
      "67.02% complete\n",
      "67.96% complete\n",
      "68.91% complete\n",
      "69.85% complete\n",
      "70.79% complete\n",
      "71.74% complete\n",
      "72.68% complete\n",
      "73.63% complete\n",
      "74.57% complete\n",
      "75.51% complete\n",
      "76.46% complete\n",
      "77.4% complete\n",
      "78.35% complete\n",
      "79.29% complete\n",
      "80.23% complete\n",
      "81.18% complete\n",
      "82.12% complete\n",
      "83.07% complete\n",
      "84.01% complete\n",
      "84.95% complete\n",
      "85.9% complete\n",
      "86.84% complete\n",
      "87.79% complete\n",
      "88.73% complete\n",
      "89.67% complete\n",
      "90.62% complete\n",
      "91.56% complete\n",
      "92.51% complete\n",
      "93.45% complete\n",
      "94.39% complete\n",
      "95.34% complete\n",
      "96.28% complete\n",
      "97.22% complete\n",
      "98.17% complete\n",
      "99.11% complete\n",
      "100%% complete\n"
     ]
    }
   ],
   "source": [
    "#add aggregate tfidf to data\n",
    "event_data_tfidf = pd.DataFrame()\n",
    "\n",
    "for i,doc in event_data.iterrows():\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        complete = round((i/event_data.shape[0])*100, 2)\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    tfs = train_matrix_cnt[i].data\n",
    "    tfidfs = train_matrix_tfidf[i].data\n",
    "    t_distinct = len(tfs)\n",
    "    t_sum = tfs.sum()\n",
    "    tfidf_sum = tfidfs.sum()\n",
    "    tfidf_mean = (0 if tfidf_sum == 0 else tfidfs.mean())\n",
    "\n",
    "    row = [t_distinct, t_sum, tfidf_sum, tfidf_mean]\n",
    "    row = pd.Series(row, index=['t_distinct', 't_sum', 'tfidf_sum', 'tfidf_mean'])\n",
    "    row = pd.DataFrame(row).T\n",
    "    event_data_tfidf = event_data_tfidf.append(row, ignore_index=True)\n",
    "    \n",
    "event_data = pd.merge(event_data, event_data_tfidf, left_index=True, right_index=True)\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete\n",
      "0.94% complete\n",
      "1.89% complete\n",
      "2.83% complete\n",
      "3.78% complete\n",
      "4.72% complete\n",
      "5.66% complete\n",
      "6.61% complete\n",
      "7.55% complete\n",
      "8.5% complete\n",
      "9.44% complete\n",
      "10.38% complete\n",
      "11.33% complete\n",
      "12.27% complete\n",
      "13.22% complete\n",
      "14.16% complete\n",
      "15.1% complete\n",
      "16.05% complete\n",
      "16.99% complete\n",
      "17.93% complete\n",
      "18.88% complete\n",
      "19.82% complete\n",
      "20.77% complete\n",
      "21.71% complete\n",
      "22.65% complete\n",
      "23.6% complete\n",
      "24.54% complete\n",
      "25.49% complete\n",
      "26.43% complete\n",
      "27.37% complete\n",
      "28.32% complete\n",
      "29.26% complete\n",
      "30.21% complete\n",
      "31.15% complete\n",
      "32.09% complete\n",
      "33.04% complete\n",
      "33.98% complete\n",
      "34.93% complete\n",
      "35.87% complete\n",
      "36.81% complete\n",
      "37.76% complete\n",
      "38.7% complete\n",
      "39.65% complete\n",
      "40.59% complete\n",
      "41.53% complete\n",
      "42.48% complete\n",
      "43.42% complete\n",
      "44.36% complete\n",
      "45.31% complete\n",
      "46.25% complete\n",
      "47.2% complete\n",
      "48.14% complete\n",
      "49.08% complete\n",
      "50.03% complete\n",
      "50.97% complete\n",
      "51.92% complete\n",
      "52.86% complete\n",
      "53.8% complete\n",
      "54.75% complete\n",
      "55.69% complete\n",
      "56.64% complete\n",
      "57.58% complete\n",
      "58.52% complete\n",
      "59.47% complete\n",
      "60.41% complete\n",
      "61.36% complete\n",
      "62.3% complete\n",
      "63.24% complete\n",
      "64.19% complete\n",
      "65.13% complete\n",
      "66.08% complete\n",
      "67.02% complete\n",
      "67.96% complete\n",
      "68.91% complete\n",
      "69.85% complete\n",
      "70.79% complete\n",
      "71.74% complete\n",
      "72.68% complete\n",
      "73.63% complete\n",
      "74.57% complete\n",
      "75.51% complete\n",
      "76.46% complete\n",
      "77.4% complete\n",
      "78.35% complete\n",
      "79.29% complete\n",
      "80.23% complete\n",
      "81.18% complete\n",
      "82.12% complete\n",
      "83.07% complete\n",
      "84.01% complete\n",
      "84.95% complete\n",
      "85.9% complete\n",
      "86.84% complete\n",
      "87.79% complete\n",
      "88.73% complete\n",
      "89.67% complete\n",
      "90.62% complete\n",
      "91.56% complete\n",
      "92.51% complete\n",
      "93.45% complete\n",
      "94.39% complete\n",
      "95.34% complete\n",
      "96.28% complete\n",
      "97.22% complete\n",
      "98.17% complete\n",
      "99.11% complete\n",
      "100%% complete\n"
     ]
    }
   ],
   "source": [
    "#calculate and add parts of speech, named entities info to data\n",
    "event_data_nespos = pd.DataFrame()\n",
    "\n",
    "#define function for nltk tree mining\n",
    "def getnes(tree):\n",
    "    ne = []\n",
    "    for node in tree:\n",
    "        if type(node) is nltk.Tree:\n",
    "            label = node.label()\n",
    "            s = ''\n",
    "            for node in node:\n",
    "                s = (s + ' ' + node[0].lower()).lstrip()\n",
    "            ne.append([label, s])\n",
    "    return ne\n",
    "\n",
    "for i,doc in event_data.iterrows():\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        complete = round((i/event_data.shape[0])*100, 2)\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(str(doc['text_clean']))\n",
    "    \n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    pos_cntr = Counter(list(dict(pos).values()))\n",
    "    pos_data = dict(pos_cntr)\n",
    "    pos_cnt = sum(pos_data.values())\n",
    "    \n",
    "    tree = nltk.ne_chunk(pos)\n",
    "    nes = getnes(tree)\n",
    "    nes_cntr = Counter(list(dict(nes).keys()))\n",
    "    nes_data = dict(nes_cntr)\n",
    "    nes_cnt = sum(nes_data.values())\n",
    "    \n",
    "    row = [pos_cnt, nes_cnt, pos_data, nes_data]\n",
    "    row = pd.Series(row, index=['pos_cnt', 'nes_cnt', 'pos_data', 'nes_data'])\n",
    "    row = pd.DataFrame(row).T\n",
    "    event_data_nespos = event_data_nespos.append(row, ignore_index=True)\n",
    "    \n",
    "event_data = pd.merge(event_data, event_data_nespos, left_index=True, right_index=True)\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete\n",
      "0.94% complete\n",
      "1.89% complete\n",
      "2.83% complete\n",
      "3.78% complete\n",
      "4.72% complete\n",
      "5.66% complete\n",
      "6.61% complete\n",
      "7.55% complete\n",
      "8.5% complete\n",
      "9.44% complete\n",
      "10.38% complete\n",
      "11.33% complete\n",
      "12.27% complete\n",
      "13.22% complete\n",
      "14.16% complete\n",
      "15.1% complete\n",
      "16.05% complete\n",
      "16.99% complete\n",
      "17.93% complete\n",
      "18.88% complete\n",
      "19.82% complete\n",
      "20.77% complete\n",
      "21.71% complete\n",
      "22.65% complete\n",
      "23.6% complete\n",
      "24.54% complete\n",
      "25.49% complete\n",
      "26.43% complete\n",
      "27.37% complete\n",
      "28.32% complete\n",
      "29.26% complete\n",
      "30.21% complete\n",
      "31.15% complete\n",
      "32.09% complete\n",
      "33.04% complete\n",
      "33.98% complete\n",
      "34.93% complete\n",
      "35.87% complete\n",
      "36.81% complete\n",
      "37.76% complete\n",
      "38.7% complete\n",
      "39.65% complete\n",
      "40.59% complete\n",
      "41.53% complete\n",
      "42.48% complete\n",
      "43.42% complete\n",
      "44.36% complete\n",
      "45.31% complete\n",
      "46.25% complete\n",
      "47.2% complete\n",
      "48.14% complete\n",
      "49.08% complete\n",
      "50.03% complete\n",
      "50.97% complete\n",
      "51.92% complete\n",
      "52.86% complete\n",
      "53.8% complete\n",
      "54.75% complete\n",
      "55.69% complete\n",
      "56.64% complete\n",
      "57.58% complete\n",
      "58.52% complete\n",
      "59.47% complete\n",
      "60.41% complete\n",
      "61.36% complete\n",
      "62.3% complete\n",
      "63.24% complete\n",
      "64.19% complete\n",
      "65.13% complete\n",
      "66.08% complete\n",
      "67.02% complete\n",
      "67.96% complete\n",
      "68.91% complete\n",
      "69.85% complete\n",
      "70.79% complete\n",
      "71.74% complete\n",
      "72.68% complete\n",
      "73.63% complete\n",
      "74.57% complete\n",
      "75.51% complete\n",
      "76.46% complete\n",
      "77.4% complete\n",
      "78.35% complete\n",
      "79.29% complete\n",
      "80.23% complete\n",
      "81.18% complete\n",
      "82.12% complete\n",
      "83.07% complete\n",
      "84.01% complete\n",
      "84.95% complete\n",
      "85.9% complete\n",
      "86.84% complete\n",
      "87.79% complete\n",
      "88.73% complete\n",
      "89.67% complete\n",
      "90.62% complete\n",
      "91.56% complete\n",
      "92.51% complete\n",
      "93.45% complete\n",
      "94.39% complete\n",
      "95.34% complete\n",
      "96.28% complete\n",
      "97.22% complete\n",
      "98.17% complete\n",
      "99.11% complete\n",
      "100%% complete\n"
     ]
    }
   ],
   "source": [
    "#get all parts of speach\n",
    "all_pos = []\n",
    "for i,doc in event_data.iterrows():\n",
    "    all_pos.extend(list(doc['pos_data'].keys()))\n",
    "\n",
    "#get unique\n",
    "all_pos = list(set(all_pos))\n",
    "\n",
    "#create pos feature df\n",
    "event_data_pos = pd.DataFrame()\n",
    "pos_cols = ['pos_cnt_'+pos for pos in all_pos] \n",
    "\n",
    "#update pos counts\n",
    "for i,doc in event_data.iterrows():    \n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        complete = round((i/event_data.shape[0])*100, 2)\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    #create empty dictionary with keys\n",
    "    pos_dict = dict.fromkeys(all_pos)\n",
    "    \n",
    "    for pos in doc['pos_data'].keys():\n",
    "        pos_dict[pos] = doc['pos_data'].get(pos)\n",
    "    \n",
    "    row = list(pos_dict.values())\n",
    "    row = pd.Series(row, index=pos_cols)\n",
    "    row = pd.DataFrame(row).T\n",
    "    event_data_pos = event_data_pos.append(row, ignore_index=True)\n",
    "    \n",
    "event_data = pd.merge(event_data, event_data_pos, left_index=True, right_index=True)\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% complete\n",
      "0.94% complete\n",
      "1.89% complete\n",
      "2.83% complete\n",
      "3.78% complete\n",
      "4.72% complete\n",
      "5.66% complete\n",
      "6.61% complete\n",
      "7.55% complete\n",
      "8.5% complete\n",
      "9.44% complete\n",
      "10.38% complete\n",
      "11.33% complete\n",
      "12.27% complete\n",
      "13.22% complete\n",
      "14.16% complete\n",
      "15.1% complete\n",
      "16.05% complete\n",
      "16.99% complete\n",
      "17.93% complete\n",
      "18.88% complete\n",
      "19.82% complete\n",
      "20.77% complete\n",
      "21.71% complete\n",
      "22.65% complete\n",
      "23.6% complete\n",
      "24.54% complete\n",
      "25.49% complete\n",
      "26.43% complete\n",
      "27.37% complete\n",
      "28.32% complete\n",
      "29.26% complete\n",
      "30.21% complete\n",
      "31.15% complete\n",
      "32.09% complete\n",
      "33.04% complete\n",
      "33.98% complete\n",
      "34.93% complete\n",
      "35.87% complete\n",
      "36.81% complete\n",
      "37.76% complete\n",
      "38.7% complete\n",
      "39.65% complete\n",
      "40.59% complete\n",
      "41.53% complete\n",
      "42.48% complete\n",
      "43.42% complete\n",
      "44.36% complete\n",
      "45.31% complete\n",
      "46.25% complete\n",
      "47.2% complete\n",
      "48.14% complete\n",
      "49.08% complete\n",
      "50.03% complete\n",
      "50.97% complete\n",
      "51.92% complete\n",
      "52.86% complete\n",
      "53.8% complete\n",
      "54.75% complete\n",
      "55.69% complete\n",
      "56.64% complete\n",
      "57.58% complete\n",
      "58.52% complete\n",
      "59.47% complete\n",
      "60.41% complete\n",
      "61.36% complete\n",
      "62.3% complete\n",
      "63.24% complete\n",
      "64.19% complete\n",
      "65.13% complete\n",
      "66.08% complete\n",
      "67.02% complete\n",
      "67.96% complete\n",
      "68.91% complete\n",
      "69.85% complete\n",
      "70.79% complete\n",
      "71.74% complete\n",
      "72.68% complete\n",
      "73.63% complete\n",
      "74.57% complete\n",
      "75.51% complete\n",
      "76.46% complete\n",
      "77.4% complete\n",
      "78.35% complete\n",
      "79.29% complete\n",
      "80.23% complete\n",
      "81.18% complete\n",
      "82.12% complete\n",
      "83.07% complete\n",
      "84.01% complete\n",
      "84.95% complete\n",
      "85.9% complete\n",
      "86.84% complete\n",
      "87.79% complete\n",
      "88.73% complete\n",
      "89.67% complete\n",
      "90.62% complete\n",
      "91.56% complete\n",
      "92.51% complete\n",
      "93.45% complete\n",
      "94.39% complete\n",
      "95.34% complete\n",
      "96.28% complete\n",
      "97.22% complete\n",
      "98.17% complete\n",
      "99.11% complete\n",
      "100%% complete\n"
     ]
    }
   ],
   "source": [
    "#get all named entities\n",
    "all_nes = []\n",
    "for i,doc in event_data.iterrows():\n",
    "    all_nes.extend(list(doc['nes_data'].keys()))\n",
    "\n",
    "#get unique\n",
    "all_nes = list(set(all_nes))\n",
    "\n",
    "#create nes feature df\n",
    "event_data_nes = pd.DataFrame()\n",
    "ne_cols = ['ne_cnt_'+ne for ne in all_nes] \n",
    "    \n",
    "#update pos counts\n",
    "for i,doc in event_data.iterrows():    \n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        complete = round((i/event_data.shape[0])*100, 2)\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    #create empty dictionary with keys\n",
    "    nes_dict = dict.fromkeys(all_nes)\n",
    "    \n",
    "    for ne in doc['nes_data'].keys():\n",
    "        nes_dict[ne] = doc['nes_data'].get(ne)\n",
    "    \n",
    "    row = list(nes_dict.values())\n",
    "    row = pd.Series(row, index=ne_cols)\n",
    "    row = pd.DataFrame(row).T\n",
    "    event_data_nes = event_data_nes.append(row, ignore_index=True)\n",
    "    \n",
    "event_data = pd.merge(event_data, event_data_nes, left_index=True, right_index=True)\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#na as 0\n",
    "event_data.to_csv('data/%s_data_clean_features.txt' % event_name, sep='\\t', encoding='utf-8', header=True, index=False, na_rep=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
